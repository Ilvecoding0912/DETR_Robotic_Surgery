{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ilvecoding0912/DETR_Robotic_Surgery/blob/main/DETR_training_v1_full_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DETR training process\n"
      ],
      "metadata": {
        "id": "udwFrw6ZiYP6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone DETR Repo after changing to our dataset"
      ],
      "metadata": {
        "id": "d9AQWNd9gKQa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pax7w1zmgEJ0",
        "outputId": "8e33a387-1995-4c4a-dcf3-2dd56fbfa4e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DETR_Robotic_Surgery'...\n",
            "remote: Enumerating objects: 121, done.\u001b[K\n",
            "remote: Counting objects: 100% (41/41), done.\u001b[K\n",
            "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
            "remote: Total 121 (delta 10), reused 0 (delta 0), pack-reused 80\u001b[K\n",
            "Receiving objects: 100% (121/121), 2.49 MiB | 21.80 MiB/s, done.\n",
            "Resolving deltas: 100% (28/28), done.\n",
            "/content/DETR_Robotic_Surgery\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Ilvecoding0912/DETR_Robotic_Surgery.git\n",
        "%cd DETR_Robotic_Surgery"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download DETR weights"
      ],
      "metadata": {
        "id": "npD6VBwsxc3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1HV2Tit0CsVeYKHugjx8QxROPegQa3AV-'\n",
        "gdown.download(url,'detr_weights.pth',quiet=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zD1ubNUGtKKi",
        "outputId": "c79baf43-75b7-4a2d-e289-80b252dbf4ac"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'detr_weights.pth'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\n",
        "!pip -q install 'git+https://github.com/facebookresearch/segment-anything.git'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoglqRuWeh1v",
        "outputId": "0ddbfdf7-5de0-436c-fb63-432930e34b4c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-28 09:39:12--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.65.229.46, 18.65.229.89, 18.65.229.121, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.65.229.46|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 375042383 (358M) [binary/octet-stream]\n",
            "Saving to: ‘sam_vit_b_01ec64.pth’\n",
            "\n",
            "sam_vit_b_01ec64.pt 100%[===================>] 357.67M  43.0MB/s    in 5.6s    \n",
            "\n",
            "2023-08-28 09:39:18 (64.0 MB/s) - ‘sam_vit_b_01ec64.pth’ saved [375042383/375042383]\n",
            "\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for segment-anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download whole Endovis dataset and unzip them into folder endovis17"
      ],
      "metadata": {
        "id": "CyEftLm_a2E3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1nroWvgxBRCIx9PP0bbIOFPretpmvTVuH'\n",
        "gdown.download(url,'endovis17.zip',quiet=True)\n",
        "!unzip endovis17.zip -d endovis17"
      ],
      "metadata": {
        "id": "s4lUcwJGbAcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/DETR_Robotic_Surgery\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "from torch import nn\n",
        "from typing import Dict, List\n",
        "from util.misc import NestedTensor\n",
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
        "import torch.nn.functional as F\n",
        "from models.position_encoding import build_position_encoding\n",
        "\n",
        "class backboneSAM(nn.Module):\n",
        "\n",
        "    def __init__(self, ):\n",
        "        super().__init__()\n",
        "        sam = sam_model_registry[\"vit_b\"](checkpoint=\"sam_vit_b_01ec64.pth\")\n",
        "        #Freeze()\n",
        "        sam.eval()\n",
        "        for param in sam.image_encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.sam_encoder = sam.image_encoder\n",
        "\n",
        "\n",
        "    def forward(self, tensor_list: NestedTensor):\n",
        "        xs = OrderedDict()\n",
        "        with torch.no_grad():\n",
        "            xs['0'] = self.sam_encoder(tensor_list.tensors)\n",
        "        # print('out sam encoder', xs['0'].shape)\n",
        "        out: Dict[str, NestedTensor] = {}\n",
        "        for name, x in xs.items():\n",
        "            m = tensor_list.mask\n",
        "            # print('m', m.shape)\n",
        "            assert m is not None\n",
        "            import torch.nn.functional as F\n",
        "            mask = F.interpolate(m[None].float(), size=x.shape[-2:]).to(torch.bool)[0]\n",
        "            # mask = F.upsample_nearest(m[None].float(),size=x.shape[-2:]).to(torch.bool)[0]\n",
        "            # print('mobarak',name, x.shape, mask.shape)\n",
        "            out[name] = NestedTensor(x, mask)\n",
        "        return out\n",
        "\n",
        "class Joiner(nn.Sequential):\n",
        "    def __init__(self, backbone, position_embedding):\n",
        "        super().__init__(backbone, position_embedding)\n",
        "\n",
        "    def forward(self, tensor_list: NestedTensor):\n",
        "        xs = self[0](tensor_list)\n",
        "        out: List[NestedTensor] = []\n",
        "        pos = []\n",
        "        for name, x in xs.items():\n",
        "            out.append(x)\n",
        "            # position encoding\n",
        "            pos.append(self[1](x).to(x.tensors.dtype))\n",
        "\n",
        "        return out, pos\n",
        "\n",
        "def build_backbone(args):\n",
        "    position_embedding = build_position_encoding(args)\n",
        "    train_backbone = args.lr_backbone > 0\n",
        "    return_interm_layers = args.masks\n",
        "    backbone = backboneSAM()#Backbone(args.backbone, train_backbone, return_interm_layers, args.dilation)\n",
        "    model = Joiner(backbone, position_embedding)\n",
        "    # model.num_channels = backbone.num_channels\n",
        "    return model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSwpRYFPe6uB",
        "outputId": "20a54150-0bf1-40d2-9934-924716ebe0fc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DETR_Robotic_Surgery\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part of content related to training in 'main.py'.\n",
        "(We ignore the evaluation part.)"
      ],
      "metadata": {
        "id": "35SBHOQ4lXOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/DETR_Robotic_Surgery\n",
        "from main import get_args_parser\n",
        "import argparse\n",
        "import torch\n",
        "import time\n",
        "import random\n",
        "import datetime\n",
        "import json\n",
        "from torch.utils.data import DataLoader, DistributedSampler\n",
        "import util.misc as utils\n",
        "from datasets import build_dataset, get_coco_api_from_dataset\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from engine import evaluate, train_one_epoch\n",
        "from models import build_model\n",
        "from datasets.coco import *\n",
        "import os\n",
        "\n",
        "def main():\n",
        "\n",
        "    parser = argparse.ArgumentParser('DETR training and evaluation script', parents=[get_args_parser()])\n",
        "    args = parser.parse_args([])\n",
        "    args.output_dir = './outputs' # Results dir\n",
        "    args.endovis_path = 'endovis17' # Path to our dataset\n",
        "    args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    # training parameters\n",
        "    args.start_epoch = 0\n",
        "    args.epochs = 50 # total number of epoch\n",
        "    args.batch_size = 1\n",
        "\n",
        "    # create output directoty if not exist\n",
        "    if os.path.exists(args.output_dir) is False:\n",
        "        os.mkdir(args.output_dir)\n",
        "\n",
        "    utils.init_distributed_mode(args)\n",
        "    print(\"git:\\n  {}\\n\".format(utils.get_sha()))\n",
        "    print(args)\n",
        "\n",
        "    device = torch.device(args.device)\n",
        "\n",
        "    # fix the seed for reproducibility\n",
        "    seed = args.seed + utils.get_rank()\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "    # create model\n",
        "    model, criterion, postprocessors = build_model(args)\n",
        "    model.to(device)\n",
        "    # print(model)\n",
        "    model_without_ddp = model\n",
        "    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print('number of params:', n_parameters)\n",
        "\n",
        "    param_dicts = [\n",
        "        {\"params\": [p for n, p in model_without_ddp.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
        "        {\n",
        "            \"params\": [p for n, p in model_without_ddp.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
        "            \"lr\": args.lr_backbone,\n",
        "        },\n",
        "    ]\n",
        "    optimizer = torch.optim.AdamW(param_dicts, lr=args.lr,\n",
        "                                    weight_decay=args.weight_decay)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.lr_drop)\n",
        "\n",
        "    # Our dataset class (initialize in datasets->coco.py)\n",
        "    dataset_train = EnvidosDataset(args.endovis_path, transforms=make_coco_transforms('train'), mode='train')\n",
        "    dataset_val = EnvidosDataset(args.endovis_path, transforms=make_coco_transforms('val'), mode='val')\n",
        "\n",
        "    if args.distributed:\n",
        "        sampler_train = DistributedSampler(dataset_train)\n",
        "        sampler_val = DistributedSampler(dataset_val, shuffle=False)\n",
        "    else:\n",
        "        sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
        "        sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
        "\n",
        "    batch_sampler_train = torch.utils.data.BatchSampler(\n",
        "        sampler_train, args.batch_size, drop_last=True)\n",
        "\n",
        "    data_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train,\n",
        "                                    collate_fn=utils.collate_fn, num_workers=args.num_workers)\n",
        "    data_loader_val = DataLoader(dataset_val, args.batch_size, sampler=sampler_val,\n",
        "                                 drop_last=False, collate_fn=utils.collate_fn, num_workers=args.num_workers)\n",
        "    #---------------------- Training Process ----------------------\n",
        "    print(\"Start training\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    sam_backbone = build_backbone(args)#BackboneBase()\n",
        "    sam_backbone.to(device)\n",
        "    sam_backbone.eval()\n",
        "    model_without_ddp.backbone = sam_backbone\n",
        "    model_without_ddp.input_proj = nn.Conv2d(256, model_without_ddp.transformer.d_model, kernel_size=1)\n",
        "    model_without_ddp.to(device)\n",
        "\n",
        "    for epoch in range(args.start_epoch, args.epochs):\n",
        "        if args.distributed:\n",
        "            sampler_train.set_epoch(epoch)\n",
        "\n",
        "        #----------------- main training function (can be seen in 'engine.py') -----------------\n",
        "        train_stats = train_one_epoch(\n",
        "            model_without_ddp, criterion, data_loader_train, optimizer, device, epoch,\n",
        "            args.clip_max_norm)\n",
        "\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        # Save trained models\n",
        "        output_dir = Path(args.output_dir)\n",
        "        if args.output_dir:\n",
        "            checkpoint_paths = [output_dir / f'checkpoint{epoch:04}.pth']\n",
        "            if (epoch + 1) % args.lr_drop == 0 or (epoch + 1) % 5 == 0:\n",
        "                checkpoint_paths.append(output_dir / f'checkpoint{epoch:04}_lr{args.lr}.pth')\n",
        "            for checkpoint_path in checkpoint_paths:\n",
        "                utils.save_on_master({\n",
        "                    'model': model_without_ddp.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'lr_scheduler': lr_scheduler.state_dict(),\n",
        "                    'epoch': epoch,\n",
        "                    'args': args,\n",
        "                }, checkpoint_path)\n",
        "        test_stats, coco_evaluator = evaluate(\n",
        "            model_without_ddp, criterion, postprocessors, data_loader_val, base_ds, device, args.output_dir, epoch\n",
        "        )\n",
        "\n",
        "        log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
        "                     **{f'test_{k}': v for k, v in test_stats.items()},\n",
        "                     'epoch': epoch,\n",
        "                     'n_parameters': n_parameters}\n",
        "\n",
        "        if args.output_dir and utils.is_main_process():\n",
        "            with (output_dir / \"log.txt\").open(\"a\") as f:\n",
        "                f.write(json.dumps(log_stats) + \"\\n\")\n",
        "\n",
        "            # for evaluation logs\n",
        "            if coco_evaluator is not None:\n",
        "                (output_dir / 'eval').mkdir(exist_ok=True)\n",
        "                if \"bbox\" in coco_evaluator.coco_eval:\n",
        "                    filenames = ['latest.pth']\n",
        "                    if epoch % 50 == 0:\n",
        "                        filenames.append(f'{epoch:03}.pth')\n",
        "                    for name in filenames:\n",
        "                        torch.save(coco_evaluator.coco_eval[\"bbox\"].eval,\n",
        "                                   output_dir / \"eval\" / name)\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "    print('Training time {}'.format(total_time_str))\n",
        "\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "UkQzsVvBgdHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as T\n",
        "import numpy as np\n",
        "\n",
        "invTrans = T.Compose([ T.Normalize(mean = [ 0., 0., 0. ],\n",
        "                                                     std = [ 1/0.229, 1/0.224, 1/0.225 ]),\n",
        "                                T.Normalize(mean = [ -0.485, -0.456, -0.406 ],\n",
        "                                                     std = [ 1., 1., 1. ]),\n",
        "                               ])\n",
        "\n",
        "img1 = invTrans(samples.tensors)\n",
        "print(img1.shape)\n",
        "print(img1[0].permute(1,2,0).shape)\n",
        "img = img1[0].permute(1,2,0).cpu()\n",
        "# ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "\n",
        "print(img.max(), img.min())\n",
        "img_cv = np.array(img*255).astype(np.uint8)\n",
        "im_rgb = cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB)\n",
        "cv2.imwrite('sample.png', im_rgb)"
      ],
      "metadata": {
        "id": "H_h_FXaZ5KNC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}